{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys, gc\n",
    "import shutil\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "model_top_guesses [2 1 1 2]\n",
      "model_second_guesses [1 0 0 1]\n",
      "human_top_guesses [2 1 2 1]\n",
      "human_second_guesses [1 2 0 0]\n",
      "matches [ True  True False False]\n",
      "overall model accuracy:  0.5\n",
      "correct total:  2\n",
      "incorrect total:  2\n",
      "{'mean_accuracy': 0.5, 'total_sba': 0.5, 'correct_sba': 0.5, 'incorrect_sba': 0.5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model_scores(model_predictions, human_predictions):\n",
    "    \"\"\"Returns second best accuracy scores\"\"\"\n",
    "    total = model_predictions.shape[0]\n",
    "\n",
    "    model_guesses = np.argsort(model_predictions, axis = 1)[:, -2: ]\n",
    "    model_second_guesses = model_guesses[:, 0]\n",
    "    model_top_guesses = model_guesses[:, 1]\n",
    "    \n",
    "    human_guesses = np.argsort(human_predictions, axis = 1)[:, -2: ]\n",
    "    human_second_guesses = human_guesses[:, 0]\n",
    "    human_top_guesses = human_guesses[:, 1]\n",
    "                                      \n",
    "    matches = model_top_guesses == human_top_guesses\n",
    "    print('total', total)\n",
    "    print('model_top_guesses', model_top_guesses)\n",
    "    print('model_second_guesses', model_second_guesses)\n",
    "    print('human_top_guesses', human_top_guesses)\n",
    "    print('human_second_guesses', human_second_guesses)\n",
    "    print('matches', matches)\n",
    "    accuracy = np.sum(matches)\n",
    "    mean_accuracy = np.mean(matches)\n",
    "    correct_total = accuracy\n",
    "    incorrect_total = total - accuracy\n",
    "    \n",
    "    print('overall model accuracy: ', mean_accuracy)\n",
    "    print('correct total: ', correct_total)\n",
    "    print('incorrect total: ', incorrect_total)\n",
    "\n",
    "    incorrect_sba = 0\n",
    "    correct_sba = 0\n",
    "    total_sba = 0\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        model_first = model_top_guesses[i]\n",
    "        human_first = human_top_guesses[i]\n",
    "        model_second = model_second_guesses[i]\n",
    "        human_second = human_second_guesses[i]\n",
    "        \n",
    "        total_sba += model_second == human_second\n",
    "        \n",
    "        if match == True:\n",
    "            correct_sba += model_second == human_second\n",
    "        elif match == False:\n",
    "            incorrect_sba += model_second == human_first\n",
    "            \n",
    "    return {'mean_accuracy': mean_accuracy, \n",
    "            'total_sba': total_sba / total, \n",
    "            'correct_sba': correct_sba / correct_total, \n",
    "            'incorrect_sba': incorrect_sba / incorrect_total}\n",
    "#                 # both c     #top c     # top i     # both i\n",
    "humans = np.array([[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1]])\n",
    "models = np.array([[1, 2, 3], [2, 3, 1], [2, 3, 1], [1, 2, 3]])\n",
    "\n",
    "# should be 2 /4 (50%) correct, 2 /4 (50%) total sba, \n",
    "#1 /2 (50%) correct sba, 1 /2 (50%) incorrect sba\n",
    "print(get_model_scores(models, humans))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
