

11/11/18 -----------------------------------------------------------------------------------
We decided we probably don't need epoch-by-epoch weights anymore since we have the
csv files with all scores for all training (tuning) and validation sets. In it's place, Josh
implemented new code in 'tune_with_cifar10h_v2.py' to save the current/best model for each 
dataset in case it's needed later for adversarial attack analyses. In this file, update_state 
behaves identically when given a single numeric score. When given a dictionary, it processes
all scores. The only difference is that in both cases best_score values don't have to be 
initialized anymore (more robustly dealing with the issue fix in the last entry in this log).
save_checkpoints_for_all_datasets is the new saving function, separate from the old function 
which still exists, but called only for human_tune. Tested and the code and seems to work.


11/11/18 -----------------------------------------------------------------------------------
mixup_250_run1 has the problem described below, so don't used the weights, only the CSVs.


11/11/18 -----------------------------------------------------------------------------------
Today and yesterday we had been running tuning with a "fix" for saving the best
model based on -c1h_val_loss (negative loss) instead of c1h_val_acc (accuracy), because
accuracy isn't what we optimize in our human tuning models (top 1 is already maxed). We
just found a bug where 'best_accuracy' in state was initialized at 0, so -loss is never 
registered as better than the first model that's loaded before tuning. This was fixed in
all tuning scripts by initializing state['best_accuracy'] to -99999999.


11/11/18 -----------------------------------------------------------------------------------
I found a comma in the learning rate list in the bash file. The models seem to have
trained fine, but it would be a good idea to check the table generation code to make sure the
comma in the folder names doesn't mess up the parsing


11/10/18 -----------------------------------------------------------------------------------
For the ICLR draft, we realized wide resnet (wrn) with cutout was just another run of
wrn without cutout, so we decided to just not include the cutout version since it 
we don't do more than one model for other architecture types anyway. We did decide to
put resnext back in since we reran the training for all models and it should now be
performing find (didn't check yet)


--------------------------------------------------------------------------------------------
--- POST ICLR reviews / CVPR VERSION -------------------------------------------------------
--------------------------------------------------------------------------------------------