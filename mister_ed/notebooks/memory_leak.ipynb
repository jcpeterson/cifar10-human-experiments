{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Okay, why is there a memory leak with my lpips stuff?\"\"\"\n",
    "# Universal import block \n",
    "# Block to get the relative imports working \n",
    "import os\n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import config\n",
    "import prebuilt_loss_functions as plf\n",
    "import loss_functions as lf \n",
    "import utils.pytorch_utils as utils\n",
    "import utils.image_utils as img_utils\n",
    "import cifar10.cifar_loader as cifar_loader\n",
    "import cifar10.cifar_resnets as cifar_resnets\n",
    "import adversarial_attacks as aa\n",
    "import adversarial_training as advtrain\n",
    "import adversarial_evaluation as adveval\n",
    "import checkpoints\n",
    "from torch.autograd import Variable\n",
    "import lpips.dist_model as dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "EPoch 0\n",
      "EPoch 1\n",
      "EPoch 2\n",
      "EPoch 3\n",
      "EPoch 4\n",
      "EPoch 5\n",
      "EPoch 6\n",
      "EPoch 7\n",
      "EPoch 8\n",
      "EPoch 9\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Forward breaks lpips?\n",
    "I have definitely witnessed a forward pass of my PerceptualXentropy loss function that \n",
    "causes a memout error with cuda. The goal of this block is to try and replicate that.\n",
    "\"\"\"\n",
    "\n",
    "def forward_breaks_lpips_1():\n",
    "    # General strat: get cifar loader and run arbitrary distance comparisons until we break \n",
    "    val_loader = cifar_loader.load_cifar_data('val', normalize=False, use_gpu=True)\n",
    "    \n",
    "    first_batch, _ = next(iter(val_loader))    \n",
    "    lpips_regularizer = lf.LpipsRegularization(Variable(first_batch.cuda()), use_gpu=True)\n",
    "    for i in xrange(10): # 10 epochs \n",
    "        print \"EPoch %s\" % i\n",
    "        for i, (examples, labels) in enumerate(val_loader):\n",
    "            if examples.shape[0] != first_batch.shape[0]:\n",
    "                continue\n",
    "            examples = Variable(examples.cuda(), requires_grad=True)\n",
    "            # print examples.shape\n",
    "            fix_im = lpips_regularizer.fix_im\n",
    "            torch.sum(lpips_regularizer.dist_model.forward_var(2*examples -1 , 2*fix_im-1))\n",
    "            out = torch.sum(lpips_regularizer.forward(examples))\n",
    "            out.backward()\n",
    "            \n",
    "            #_ = lpips_regularizer.forward(examples)\n",
    "            \n",
    "\n",
    "\n",
    "forward_breaks_lpips_1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Variable containing:\n",
      " 0.5658\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  7.2645e-05  1.5882e-04  5.7358e-05  ...  -1.0450e-04 -2.2553e-05 -8.1600e-06\n",
      "  1.1721e-04  1.3188e-04 -7.3912e-06  ...  -2.2492e-04 -1.9046e-05  1.8680e-05\n",
      " -4.6280e-05  1.4303e-04  7.9768e-05  ...  -3.7147e-05  1.4194e-04  1.2253e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -8.0557e-05 -1.6988e-04 -3.8638e-04  ...  -2.0251e-04 -2.0716e-04 -5.7161e-05\n",
      "  1.0974e-04  1.7803e-04  2.7093e-04  ...  -1.9485e-04 -1.6650e-04 -1.2187e-05\n",
      " -1.2900e-04 -1.6780e-04 -1.5961e-04  ...  -1.4606e-04 -1.0146e-04  1.8680e-05\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -2.5180e-05  1.1779e-04  2.5061e-04  ...  -1.7541e-04 -8.0358e-05 -3.9741e-05\n",
      " -3.4342e-05  5.6369e-05  1.8414e-04  ...  -3.1962e-04 -8.9423e-05 -2.5685e-05\n",
      " -2.9653e-04 -1.1740e-05  1.8767e-04  ...  -2.7568e-04 -5.2932e-05 -6.0643e-06\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.9681e-04 -4.0779e-04 -5.7353e-04  ...   2.7085e-04  3.0505e-04  2.0938e-04\n",
      " -1.6474e-05  1.0527e-04  3.8816e-04  ...   2.8166e-04  3.1623e-04  2.4389e-04\n",
      " -2.0877e-04 -2.0284e-04 -5.2456e-05  ...   1.5742e-04  1.9039e-04  1.6046e-04\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -1.2616e-04  4.3178e-05  8.5256e-05  ...  -4.8780e-05  6.7059e-06 -3.0619e-06\n",
      " -9.8203e-05  5.8217e-05  8.8889e-05  ...  -1.5080e-04 -4.6787e-06 -6.9373e-06\n",
      " -2.6753e-04  6.9135e-05  1.6244e-04  ...   3.0079e-05  1.6120e-04  1.4229e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.3233e-04 -1.7689e-04 -2.4781e-04  ...  -1.1586e-04 -1.0533e-04 -2.7063e-04\n",
      "  3.9114e-05  1.4100e-04  2.8480e-04  ...  -8.2442e-05 -1.0280e-04 -2.6923e-04\n",
      " -1.6208e-04 -1.9042e-04 -1.9838e-04  ...  -1.8618e-04 -2.3164e-04 -3.6737e-04\n",
      "     ⋮ \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -1.1692e-04 -1.7026e-04 -5.6544e-04  ...  -1.4224e-04  3.5323e-04  7.1857e-04\n",
      " -1.7517e-04 -3.5487e-05  9.5430e-05  ...  -5.9970e-05 -7.5969e-05  1.5725e-04\n",
      "  9.8063e-06  2.7299e-04 -8.6573e-05  ...   2.4005e-04 -2.6065e-04 -5.1578e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.4307e-06 -1.4509e-04 -1.9314e-04  ...  -1.0308e-04 -1.1639e-04 -1.6623e-04\n",
      "  5.8622e-05  8.9515e-05  1.5773e-04  ...  -1.1333e-04 -1.0046e-04 -1.2045e-05\n",
      "  8.8382e-05  8.2030e-05  2.4372e-04  ...  -1.2189e-04 -4.4340e-05  5.3981e-07\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      " -1.8650e-04 -1.4973e-04 -2.1819e-04  ...  -3.3016e-04  1.8288e-04  5.9324e-04\n",
      " -1.1689e-04  1.0816e-04  3.6381e-04  ...  -2.8402e-04 -4.1075e-04 -1.7711e-04\n",
      " -2.1334e-04  2.1865e-04 -2.9492e-05  ...  -1.0377e-04 -7.8706e-04 -1.1328e-03\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.6384e-04  1.4992e-04 -7.3296e-05  ...  -9.2857e-05  4.2246e-05 -2.1229e-04\n",
      " -1.0609e-04  1.0195e-04 -1.2078e-05  ...  -8.9116e-05  1.1219e-05 -1.5264e-04\n",
      " -3.4318e-04 -2.3238e-04 -2.3582e-04  ...  -2.8213e-04 -2.0783e-04 -3.1598e-04\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      " -2.1690e-04 -2.4259e-04 -1.1408e-04  ...  -1.9259e-04  1.8133e-04  3.8702e-04\n",
      " -2.4993e-04 -1.7763e-04  1.4525e-04  ...  -5.0003e-05 -1.3546e-04 -5.5440e-05\n",
      " -5.2934e-04 -1.1787e-04 -2.9565e-04  ...   7.3296e-05 -4.7870e-04 -9.1309e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -4.7623e-04 -5.1386e-04 -6.0000e-04  ...   7.7876e-05  2.0214e-04 -2.3455e-04\n",
      " -4.3679e-04 -1.9711e-04 -1.7518e-04  ...   7.4335e-05  3.8398e-05 -2.4771e-04\n",
      " -5.5739e-04 -3.6741e-04 -1.8478e-04  ...  -2.5906e-04 -2.8005e-04 -5.5092e-04\n",
      "     ⋮ \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -2.9024e-04 -3.5550e-04 -2.3619e-04  ...  -5.5307e-04 -3.8817e-04 -2.4485e-04\n",
      " -3.3489e-04 -1.2086e-04  2.0274e-04  ...  -6.4698e-04 -5.3536e-04 -4.3069e-04\n",
      "  6.9295e-05  4.1664e-04  5.5310e-04  ...  -4.4606e-04 -4.7799e-04 -3.5110e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.0822e-04 -1.6727e-04  5.2958e-05  ...  -3.8333e-05 -6.4526e-05 -5.3583e-05\n",
      " -9.8484e-05 -1.1257e-04  7.0945e-05  ...  -1.4689e-05 -2.1254e-05 -1.3862e-05\n",
      " -4.9287e-05 -7.5140e-05 -5.1496e-05  ...   1.4529e-05  7.3128e-07 -1.7645e-07\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      " -5.0744e-04 -4.8233e-04 -3.1626e-04  ...  -3.0766e-04 -1.5946e-04 -1.2106e-04\n",
      " -5.5043e-04 -2.0855e-04  2.3007e-04  ...  -2.6846e-04 -1.8611e-04 -2.1369e-04\n",
      " -2.4988e-04  2.1183e-04  3.5410e-04  ...  -8.4786e-05 -1.2953e-04 -1.1287e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.1047e-05  9.1223e-05  1.1997e-05  ...   4.0589e-05  3.3843e-06 -1.2066e-05\n",
      "  5.1122e-05  1.1655e-04 -5.3361e-05  ...   2.8765e-05  1.7970e-05  6.6012e-06\n",
      "  8.8867e-05  1.0801e-04 -2.0650e-04  ...   1.8837e-05  9.8380e-06  7.3118e-06\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      " -3.0555e-04 -2.4537e-04 -1.8533e-04  ...  -6.6490e-05  7.9476e-05  6.2266e-05\n",
      " -3.7765e-04 -1.2683e-04  1.6522e-04  ...  -1.4330e-05  8.2177e-05 -4.0978e-05\n",
      " -1.5208e-04  2.3704e-04  3.9358e-04  ...   8.3099e-05  5.7004e-05  3.5527e-05\n",
      "                 ...                   ⋱                   ...                \n",
      "  6.3603e-06  2.0276e-05  2.8565e-04  ...  -1.4579e-05 -2.7470e-05 -3.8209e-05\n",
      "  2.4159e-05  6.4536e-05  3.3885e-04  ...  -1.0476e-05 -1.7045e-05 -2.4116e-05\n",
      "  3.1867e-05  3.3114e-05  2.1875e-04  ...  -1.0617e-05 -2.1003e-05 -3.7091e-05\n",
      "     ⋮ \n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -3.1836e-05  7.6854e-07 -8.5565e-05  ...   6.8692e-05  9.8504e-05  5.8007e-05\n",
      " -6.7024e-05 -4.0445e-05 -7.3626e-05  ...   7.8256e-05  1.1403e-04  8.8641e-05\n",
      " -1.6290e-05 -1.6960e-05 -1.4062e-04  ...   6.8954e-05  1.1285e-04  1.4988e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.1868e-04  2.7374e-04  5.8022e-04  ...   2.8840e-04  1.1535e-04  1.1320e-04\n",
      " -5.2654e-05 -4.9158e-06  1.5981e-04  ...   3.9313e-04  2.1104e-04  1.1392e-04\n",
      "  5.7353e-05 -5.0395e-05  4.8150e-05  ...   4.3447e-04  3.6545e-04  3.3642e-04\n",
      "\n",
      "(3 ,1 ,.,.) = \n",
      "  2.2476e-05  2.8931e-07 -1.1216e-04  ...   3.4980e-05  7.5101e-05  5.9255e-05\n",
      " -4.3258e-05 -3.3042e-05 -1.0138e-04  ...   3.2867e-05  5.4401e-05  3.6542e-05\n",
      " -1.8910e-05 -5.1517e-05  4.2558e-04  ...   2.1413e-05  7.5297e-05  1.5526e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  1.4560e-04 -2.5056e-04 -7.4461e-04  ...  -1.1864e-03 -8.4094e-04 -4.7642e-04\n",
      "  2.9515e-04  3.1109e-05 -2.6200e-04  ...  -4.7920e-04 -3.7634e-04 -3.4471e-04\n",
      "  3.1338e-04  2.9567e-04  1.8281e-04  ...  -1.4314e-04 -7.8176e-05 -1.3335e-04\n",
      "\n",
      "(3 ,2 ,.,.) = \n",
      "  2.3926e-04  2.1209e-04  7.9690e-05  ...  -3.5730e-06  8.2711e-06 -2.3728e-06\n",
      "  1.3852e-04  1.3030e-04  4.9054e-05  ...  -1.0756e-05 -2.0100e-05 -3.4673e-05\n",
      "  1.5709e-04  9.4716e-05 -1.1624e-04  ...  -1.4055e-05 -1.7039e-05  2.4298e-05\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.5000e-04  4.2319e-06  4.5720e-04  ...   5.8892e-04  4.7412e-04  2.2987e-04\n",
      " -1.3011e-04  2.2065e-05  6.9918e-05  ...   3.8059e-04  3.2380e-04  5.9712e-05\n",
      " -1.8967e-04 -1.0639e-04 -5.0564e-05  ...   1.3872e-04  2.2653e-04  3.3066e-05\n",
      "     ⋮ \n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -2.0066e-05  9.4206e-05 -2.8524e-05  ...   3.0464e-04  2.5124e-04  1.7482e-04\n",
      " -1.2719e-05  5.0903e-05  8.0928e-05  ...  -1.5718e-04 -1.2371e-04 -7.5401e-05\n",
      " -1.2787e-05 -1.5811e-05  3.8519e-05  ...  -1.9684e-04  2.8532e-05  1.7520e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -6.0981e-05 -1.1720e-04  4.0272e-06  ...   3.0683e-04  1.5280e-04  1.0082e-04\n",
      "  2.0642e-05  1.0856e-06 -1.9344e-04  ...   3.6674e-04  1.5102e-04 -1.1083e-05\n",
      "  4.4785e-05  1.9259e-04  7.7313e-05  ...   1.9351e-04  6.8499e-05 -5.2610e-05\n",
      "\n",
      "(4 ,1 ,.,.) = \n",
      " -8.4330e-05  4.0712e-05 -7.3764e-05  ...   3.4551e-04  2.8423e-04  1.9551e-04\n",
      " -7.3188e-05  3.2160e-05  8.6186e-06  ...  -2.2868e-04 -2.0329e-04 -1.4163e-04\n",
      " -6.9778e-06  6.9917e-05  3.6822e-05  ...  -2.5076e-04 -2.5250e-05  1.4394e-04\n",
      "                 ...                   ⋱                   ...                \n",
      " -5.7246e-05 -1.7522e-04 -8.0009e-05  ...   6.7250e-05 -2.1878e-04 -1.2985e-04\n",
      "  7.9494e-05 -3.7177e-06 -2.2720e-04  ...   9.6463e-05 -1.7837e-04 -2.0163e-04\n",
      "  8.0165e-05  2.1713e-04  1.6850e-04  ...   4.4237e-05 -7.3268e-05 -1.0651e-04\n",
      "\n",
      "(4 ,2 ,.,.) = \n",
      " -1.5441e-04 -4.9690e-05 -2.4443e-06  ...   2.5993e-04  2.1491e-04  1.7816e-04\n",
      " -1.4317e-04 -5.6828e-05 -5.3960e-06  ...  -1.1634e-04 -9.5703e-05 -4.1559e-05\n",
      " -8.9574e-05 -1.1336e-05 -2.6727e-05  ...  -1.6513e-04  5.8543e-05  1.7768e-04\n",
      "                 ...                   ⋱                   ...                \n",
      "  5.2680e-05 -6.9142e-05 -5.3829e-05  ...  -6.9331e-06 -7.9553e-05  1.9543e-04\n",
      "  1.1786e-04  6.3845e-05 -1.0669e-04  ...   1.3967e-04  5.0238e-05  1.9976e-04\n",
      "  4.7350e-05  1.8186e-04  1.8978e-04  ...   2.3183e-04  2.4277e-04  3.6497e-04\n",
      "[torch.cuda.FloatTensor of size 5x3x32x32 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# understand dist model \n",
    "\n",
    "def understand_dist_model():\n",
    "    dist_model = dm.DistModel()\n",
    "    dist_model.initialize(model='net-lin', net='alex', use_gpu=True)\n",
    "    val_loader = cifar_loader.load_cifar_data('val', normalize=False, use_gpu=True)    \n",
    "    first_batch, _ = next(iter(val_loader))    \n",
    "    first_batch = first_batch.cuda()\n",
    "    xform = lambda img: img * 2 - 1\n",
    "\n",
    "    first_stack = xform(first_batch[:5])\n",
    "    second_stack = xform(first_batch[5:10])\n",
    "    \n",
    "    make_var = lambda tensor: Variable(tensor, requires_grad=True)\n",
    "    first_var = make_var(first_stack)\n",
    "    second_var = make_var(second_stack)\n",
    "    \n",
    "    out = torch.sum(dist_model.forward_var(first_var, second_var))\n",
    "    \n",
    "    print out \n",
    "    out.backward()\n",
    "    print first_var.grad\n",
    "    \n",
    "    \n",
    "    \n",
    "understand_dist_model()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
